{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0baead1d-7b40-405b-bb4a-7efb950c6157",
   "metadata": {},
   "source": [
    "First we instantiate the simulator class we will use to generate our sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a91ec2-b74f-4154-8844-19d876af75ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stonesoup.types.groundtruth import GroundTruthPath, GroundTruthState\n",
    "from stonesoup.types.detection import Detection\n",
    "\n",
    "from stonesoup.types.track import Track\n",
    "from stonesoup.types.hypothesis import SingleHypothesis\n",
    "\n",
    "class simulator():\n",
    "\n",
    "    def __init__(self, transition_model, measurement_model, state_range, meas_range = None, noise_range = None, meas_noise_range = None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        transition_model : :class:`~.Predictor`\n",
    "            The Stone Soup predictor to be used.\n",
    "        measurement_model : :class:`~.Predictor`\n",
    "            The Updater to be used.\n",
    "        \"\"\"\n",
    "        self.transition_model= transition_model\n",
    "        self.measurement_model = measurement_model\n",
    "        self.state_range = state_range\n",
    "        self.meas_range = meas_range\n",
    "        self.noise_range = noise_range\n",
    "        self.meas_noise_range = meas_noise_range\n",
    "    \n",
    "    def _generate_ground_truth(self, prior, time_span):\n",
    "        \n",
    "        time_interval = time_span[1]-time_span[0]\n",
    "        ground_truth = GroundTruthPath([prior])\n",
    "        for k in range(1, len(time_span)):\n",
    "            ground_truth.append(GroundTruthState(\n",
    "                self.transition_model.function(ground_truth[k-1], noise=True, time_interval=time_interval),\n",
    "                timestamp=time_span[k-1]))\n",
    "        return ground_truth\n",
    "    \n",
    "    def _simulate_measurements(self, ground_truth):\n",
    "        #Simulate Measurements\n",
    "        measurements = []\n",
    "        for state in ground_truth:\n",
    "            measurement = self.measurement_model.function(state, noise=True)\n",
    "            measurements.append(Detection(measurement,\n",
    "                                          timestamp=state.timestamp,\n",
    "                                          measurement_model=self.measurement_model))\n",
    "        return measurements\n",
    "\n",
    "    def _initializer(self, time_stamp):\n",
    "        state_range_min = self.state_range[0]\n",
    "        state_range_max = self.state_range[1]\n",
    "        state_vector = StateVector(np.random.uniform(low=state_range_min, high=state_range_max))\n",
    "        meas_range_low = self.meas_range[0]\n",
    "        meas_range_max = self.meas_range[1]\n",
    "        self.measurement_model.translation_offset = np.random.uniform(low=meas_range_low, high=meas_range_max).reshape(len(meas_range_low),1)\n",
    "        return State(state_vector = state_vector, timestamp=time_stamp)\n",
    "         \n",
    "    def generate_training_data(self, time_span):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ground_truth : :class:`~.GroundTruthPath`\n",
    "            StateMutableSequence type object used to store ground truth.\n",
    "        initial_state : :class:`~.State`\n",
    "            Initial state for the ground truth system. This MUST be a State,\n",
    "            not a State subclass, like GaussianState or EnsembleState.\n",
    "        prior : :class:`~.GaussianState` or :class:`~.EnsembleState`\n",
    "            Initial state prediction of tracking algorithm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        track : :class:`~.Track`\n",
    "            The Stone Soup track object which contains the list of updated \n",
    "            state predictions made by the tracking algorithm employed.\n",
    "        \"\"\"\n",
    "\n",
    "        #Simulate Measurements\n",
    "        initial_state = self._initializer(time_span[0])\n",
    "        ground_truth = self._generate_ground_truth(initial_state, time_span)\n",
    "        measurements = self._simulate_measurements(ground_truth)\n",
    "        \n",
    "        return ground_truth, measurements\n",
    "\n",
    "    def simulate_track(self, predictor, updater, initial_state, prior, time_span):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        predictor : :class:`~.Predictor`\n",
    "            The Stone Soup predictor to be used.\n",
    "        updater : :class:`~.Predictor`\n",
    "            The Updater to be used.\n",
    "        ground_truth : :class:`~.GroundTruthPath`\n",
    "            StateMutableSequence type object used to store ground truth.\n",
    "        initial_state : :class:`~.State`\n",
    "            Initial state for the ground truth system. This MUST be a State,\n",
    "            not a State subclass, like GaussianState or EnsembleState.\n",
    "        prior : :class:`~.GaussianState` or :class:`~.EnsembleState`\n",
    "            Initial state prediction of tracking algorithm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        track : :class:`~.Track`\n",
    "            The Stone Soup track object which contains the list of updated \n",
    "            state predictions made by the tracking algorithm employed.\n",
    "        \"\"\"\n",
    "\n",
    "        #Simulate Measurements\n",
    "        ground_truth = self._generate_ground_truth(initial_state, time_span)\n",
    "        measurements = self._simulate_measurements(ground_truth)\n",
    "        \n",
    "        #Initialize Loop Variables\n",
    "        track = Track()\n",
    "        for measurement in measurements:\n",
    "            prediction = predictor.predict(prior, timestamp=measurement.timestamp)\n",
    "            hypothesis = SingleHypothesis(prediction, measurement)  # Group a prediction and measurement\n",
    "            posterior = updater.update(hypothesis)\n",
    "            track.append(posterior)\n",
    "            prior = track[-1]\n",
    "        return ground_truth, track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc4d79-f839-49f3-87f6-7ef115d95ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89b226e-0314-4d3b-ad70-5381641097e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHere we instantiate the simulator with our transition and measurement model.\\nThis class is capable of generating sets of ground truth points, and simulate\\nmeasurements for our recursive filters.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "from stonesoup.types.array import StateVector, CovarianceMatrix\n",
    "from stonesoup.types.state import State, GaussianState\n",
    "\n",
    "from stonesoup.models.transition.linear import (CombinedLinearGaussianTransitionModel,\n",
    "                                                ConstantVelocity)\n",
    "from stonesoup.models.measurement.linear import LinearGaussian\n",
    "from stonesoup.updater.kalman import KalmanUpdater, ExtendedKalmanUpdater\n",
    "from stonesoup.predictor.ensemble import EnsemblePredictor\n",
    "from stonesoup.predictor.kalman import KalmanPredictor, ExtendedKalmanPredictor\n",
    "from stonesoup.models.measurement.nonlinear import CartesianToBearingRange\n",
    "\n",
    "\"\"\"\n",
    "    This script represents the code used to gather the data used in [PAPER HERE].\n",
    "    \n",
    "    This repository is structured such that different stone soup algorithms \n",
    "    can be run rapidly. Hopefully I've made it modular enough to \n",
    "    allow swapping of things like algorithms, and \"experiments\" by replacing\n",
    "    the desired transition and measurement models.\n",
    "    \n",
    "    The simulator class requires a transition and \n",
    "    measurement model, then the simulate_track method accepts a Stone Soup\n",
    "    Predictor, Updater, ground truth initial state, initial state for the\n",
    "    chosen algorithm, and a span of time which the simulation takes place over.\n",
    "    This time span should be an evenly spaced datetime.datetime list.\n",
    "    \n",
    "    The simulator then, is used to gather \"Track\" instances, and with a list \n",
    "    of tracks, RMSE can then be calculated.\n",
    "\"\"\"\n",
    "\n",
    "i = 60\n",
    "num_vectors = i*5\n",
    "\n",
    "\"\"\"\n",
    "    Here, we get our initial variables for simulation. For this, we are just\n",
    "    using a time span of 60 time instances spaced one second apart.\n",
    "\"\"\"\n",
    "\n",
    "timestamp = datetime.datetime(2021, 4, 2, 12, 28, 57)\n",
    "tMax = 60\n",
    "dt = 1\n",
    "tRange = tMax // dt\n",
    "plot_time_span = np.array([dt*i for i in range(tRange)])\n",
    "\n",
    "time_span = np.array([timestamp + datetime.timedelta(seconds=dt*i) for i in range(tRange)])\n",
    "\n",
    "\"\"\"\n",
    "Here we instantiate our transition and measurement models. These are the \n",
    "same models used in the StoneSoup Kalman Filter examples.\n",
    "\"\"\"\n",
    "\n",
    "q_x = 0.05\n",
    "q_y = 0.05\n",
    "sensor_x = 25  # Placing the sensor off-centre\n",
    "sensor_y = -5\n",
    "\n",
    "transition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(q_x),\n",
    "                                                          ConstantVelocity(q_y)])\n",
    "\n",
    "measurement_model = CartesianToBearingRange(\n",
    "ndim_state=4,\n",
    "mapping=(0, 2),\n",
    "noise_covar=np.diag([np.radians(0.2), 1]),  # Covariance matrix. 0.2 degree variance in\n",
    "# bearing and 1 metre in range\n",
    "translation_offset=np.array([[sensor_x], [sensor_y]])  # Offset measurements to location of\n",
    "# sensor in cartesian.\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Here we instantiate the simulator with our transition and measurement model.\n",
    "This class is capable of generating sets of ground truth points, and simulate\n",
    "measurements for our recursive filters.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3c7b9-e997-4aab-a103-a504880da933",
   "metadata": {},
   "source": [
    "Now with our simulator we need to instantiate the Pytorch Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89b76935-2e6b-46e2-9712-e3bb6fb1815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Datasets should be specified on a per simulation basis\n",
    "\n",
    "#class sequence_generator(Dataset):\n",
    "\n",
    "class dataset_2D_bearing_range(Dataset):\n",
    "    #Range Bearing 2D Dataset Sequence Packer\n",
    "\n",
    "    def __init__(self, dataset, ground_truth=None, measurements=None):\n",
    "        if dataset == None:   \n",
    "            gt = np.array([e.state_vector for e in ground_truth]).squeeze().T\n",
    "            gt = torch.tensor(gt.astype(dtype=np.float32),device = device)\n",
    "            ms = np.array([m.state_vector for m in measurements]).squeeze().T\n",
    "            ms = torch.tensor(ms.astype(dtype=np.float32),device = device)\n",
    "            self.dataset = torch.unsqueeze(torch.cat((ms,gt)),dim=0)\n",
    "        else:\n",
    "            self.dataset = dataset\n",
    "            \n",
    "    \n",
    "    def append_to_dataset(self, ground_truth, measurements):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        ground_truth : :class:`~.list`\n",
    "            A list of Stonesoup States.\n",
    "        measurements : :class:`~.measurements`\n",
    "            The list of Stonesoup Measurements to be used.\n",
    "        \"\"\"\n",
    "    \n",
    "        gt = np.array([e.state_vector for e in ground_truth]).squeeze().T\n",
    "        gt = torch.tensor(gt.astype(dtype=np.float32),device = device)\n",
    "        ms = np.array([m.state_vector for m in measurements]).squeeze().T\n",
    "        ms = torch.tensor(ms.astype(dtype=np.float32),device = device)\n",
    "        new_entry = torch.cat((ms,gt),dim=0)\n",
    "\n",
    "        self.dataset = torch.cat((self.dataset,torch.unsqueeze(new_entry,dim=0)),dim=0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx,0:2].T, self.dataset[idx,2:6].T\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f5f341-04aa-4f3f-b554-c01f40df6165",
   "metadata": {},
   "source": [
    "Now we need to simulate and add to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35ce240c-0a77-4470-b1b1-5230d611b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Datasets should be specified on a per simulation basis\n",
    "\n",
    "#class sequence_generator(Dataset):\n",
    "\n",
    "class dataset_simulator():\n",
    "    #Range Bearing 2D Dataset Sequence Packer\n",
    "    def __init__(self, simulators, training_dataset, testing_dataset):\n",
    "        #pass a list of simulators\n",
    "        self.simulators = simulators\n",
    "        self.training_dataset = training_dataset\n",
    "        self.testing_dataset = testing_dataset\n",
    "\n",
    "    def simulate_trajectories(self, training_dataset_len, testing_ratio):\n",
    "        for simulator in self.simulators:\n",
    "            trainset_length = int((training_dataset_len-1) / len(self.simulators)) # Partition dataset into sets from each simulator\n",
    "            for i in range(trainset_length):\n",
    "                ground_truth, measurements = simulator.generate_training_data(time_span)\n",
    "                self.training_dataset.append_to_dataset(ground_truth, measurements)\n",
    "            trainloader = DataLoader(self.training_dataset)\n",
    "\n",
    "            testset_length = int(((training_dataset_len)*testing_ratio -1)/len(self.simulators))\n",
    "            for i in range(testset_length):\n",
    "                ground_truth, measurements = simulator.generate_training_data(time_span)\n",
    "                self.testing_dataset.append_to_dataset(ground_truth, measurements)\n",
    "            testloader = DataLoader(self.testing_dataset)\n",
    "\n",
    "        return trainloader, testloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3514489-8f13-464a-8c87-296cb40cf9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Length: 1000\n",
      "Testing Dataset Length: 10000\n"
     ]
    }
   ],
   "source": [
    "state_range_min = np.array([-25, -2, -25, -2])\n",
    "state_range_max = np.array([25, 2, 25, 2])\n",
    "\n",
    "meas_range_min = np.array([25, -5])\n",
    "meas_range_max = np.array([25, -5])\n",
    "\n",
    "nonlinear_simulator = simulator(transition_model=transition_model,\n",
    "                      measurement_model=measurement_model,\n",
    "                      state_range = (state_range_min, state_range_max),\n",
    "                      meas_range = (meas_range_min, meas_range_max)\n",
    ")\n",
    "\n",
    "ground_truth, measurements = nonlinear_simulator.generate_training_data(time_span)\n",
    "training_dataset =  dataset_2D_bearing_range(dataset = None, ground_truth = ground_truth, measurements = measurements)\n",
    "testing_dataset =  dataset_2D_bearing_range(dataset = None, ground_truth = ground_truth, measurements = measurements)\n",
    "\n",
    "training_dataset_len = 10000\n",
    "testing_ratio = 0.10\n",
    "\n",
    "dataset_simulator = dataset_simulator([nonlinear_simulator], testing_dataset, training_dataset)\n",
    "\n",
    "trainloader, testloader = dataset_simulator.simulate_trajectories(training_dataset_len, testing_ratio)\n",
    "print('Training Dataset Length:', training_dataset.__len__())\n",
    "print('Testing Dataset Length:',testing_dataset.__len__())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f298b99-0c26-4436-a6a3-bc227fd8b9fb",
   "metadata": {},
   "source": [
    "Turn Sequences into dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ad44e-5074-4c21-8bbe-7a2968ccc9f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf9492f-f6d9-4dba-b63d-390ca68b5639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 8\n",
    "num_layers = 1\n",
    "nonlinearity = 'relu'\n",
    "output_size = 4\n",
    "\n",
    "class BiDirectionalRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, nonlinearity, num_layers, device):\n",
    "        super(BiDirectionalRNN, self).__init__()\n",
    "        self.RNN = torch.nn.RNN(input_size, hidden_size, bidirectional=True, num_layers=num_layers, device=device)\n",
    "        self.linear = torch.nn.Linear(2*hidden_size, output_size, device=device)\n",
    "\n",
    "    def forward(self, input, RNN_hidden):\n",
    "        RNN_output, RNN_hidden = self.RNN(input, RNN_hidden)\n",
    "        output = self.linear(RNN_output)\n",
    "        return output, RNN_hidden\n",
    "\n",
    "model = BiDirectionalRNN(input_size, hidden_size, output_size, nonlinearity, num_layers=num_layers, device=device)\n",
    "#model = EncoderDecoderRNN(input_size, hidden_size, output_size, num_layers=num_layers, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d4ce5-9a2e-4efd-972b-4a73124ee52d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "985381c6-2a9f-435e-a58c-57de3ce79901",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.00005;\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate, weight_decay = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a3363-ee43-4566-945f-a1df775924f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 250;\n",
    "RNN_hidden = torch.zeros((2, tMax, hidden_size),device=device)\n",
    "testing_loss = np.zeros(n_epochs)\n",
    "training_loss = np.zeros(n_epochs)\n",
    "for i in range(n_epochs):\n",
    "    for j, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad(); \n",
    "        outputs, encoder_hidden = model(inputs, RNN_hidden.detach());\n",
    "        loss = criterion(outputs, labels);\n",
    "        loss.backward();\n",
    "        training_loss[i] = loss\n",
    "        optimizer.step();\n",
    "    with (torch.no_grad()):\n",
    "        for j, data in enumerate(testloader):\n",
    "            inputs, labels = data\n",
    "            outputs, encoder_hidden = model(inputs, RNN_hidden.detach());\n",
    "            loss = criterion(outputs.squeeze(), labels.squeeze());\n",
    "            testing_loss[i] = loss\n",
    "        print(\"---------------------------\",\"\\nEpoch:\",i+1, \"\\nTraining Loss:\", training_loss[i], \"\\nTesting Loss:\", testing_loss[i],\"\\n---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29efe313-64a7-47ce-9840-dfcacd5e5a75",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a2593-88fb-4386-8fb6-e94b4a0abf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0,n_epochs)\n",
    "testing_loss = np.zeros(n_epochs)\n",
    "training_loss = np.zeros(n_epochs)\n",
    "print(testing_loss.shape)\n",
    "print(x.shape)\n",
    "testing_loss[40] = 1\n",
    "\n",
    "fig, ax = plt.subplots(2,1)\n",
    "\n",
    "ax[0].set_title(\"Training Loss\")\n",
    "ax[0].plot(x,training_loss,color=\"C1\")\n",
    "fig.tight_layout()\n",
    "ax[1].set_title(\"Testing Loss\")\n",
    "ax[1].plot(x,testing_loss,color=\"C0\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc0468a-1a61-449d-8dbb-7a13666537c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5fd891-f007-4f65-868a-b085e5d40ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(np.array([[sensor_x], [sensor_y]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf290af-e5c2-45e5-9fce-df06c7278e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
