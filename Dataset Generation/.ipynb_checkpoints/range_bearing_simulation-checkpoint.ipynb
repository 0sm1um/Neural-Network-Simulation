{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0baead1d-7b40-405b-bb4a-7efb950c6157",
   "metadata": {},
   "source": [
    "First we instantiate the simulator class we will use to generate our sequences. The Simulator handles all the particulars about the target being tracked. This simulation in particular is a Linear Time Invariant scenario of an object travelling at a constant velocity in two dimensions. However, it is being observed via a nonlinear measurement model.\n",
    "\n",
    "In particular it is a Bearing Range sensor positioned at the origin of the coordinate system. Measurements are collected as a 2 dimensional vector containing an angle, and a distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a91ec2-b74f-4154-8844-19d876af75ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stonesoup.types.groundtruth import GroundTruthPath, GroundTruthState\n",
    "from stonesoup.types.detection import Detection\n",
    "\n",
    "from stonesoup.types.track import Track\n",
    "from stonesoup.types.hypothesis import SingleHypothesis\n",
    "\n",
    "class simulator():\n",
    "\n",
    "    def __init__(self, transition_model, measurement_model, state_range, meas_range = None, noise_range = None, meas_noise_range = None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        transition_model : :class:`~.Predictor`\n",
    "            The Stone Soup predictor to be used.\n",
    "        measurement_model : :class:`~.Predictor`\n",
    "            The Updater to be used.\n",
    "        \"\"\"\n",
    "        self.transition_model= transition_model\n",
    "        self.measurement_model = measurement_model\n",
    "        self.state_range = state_range\n",
    "        self.meas_range = meas_range\n",
    "        self.noise_range = noise_range\n",
    "        self.meas_noise_range = meas_noise_range\n",
    "\n",
    "    def _generate_ground_truth(self, prior, time_span):\n",
    "        \n",
    "        time_interval = time_span[1]-time_span[0]\n",
    "        ground_truth = GroundTruthPath([prior])\n",
    "        for k in range(1, len(time_span)):\n",
    "            ground_truth.append(GroundTruthState(\n",
    "                self.transition_model.function(ground_truth[k-1], noise=True, time_interval=time_interval),\n",
    "                timestamp=time_span[k-1]))\n",
    "        return ground_truth\n",
    "    \n",
    "    def _simulate_measurements(self, ground_truth):\n",
    "        #Simulate Measurements\n",
    "        measurements = []\n",
    "        for state in ground_truth:\n",
    "            measurement = self.measurement_model.function(state, noise=True)\n",
    "            measurements.append(Detection(measurement,\n",
    "                                          timestamp=state.timestamp,\n",
    "                                          measurement_model=self.measurement_model))\n",
    "        return measurements\n",
    "\n",
    "    def _initializer(self, time_stamp):\n",
    "        state_range_min = self.state_range[0]\n",
    "        state_range_max = self.state_range[1]\n",
    "        state_vector = StateVector(np.random.uniform(low=state_range_min, high=state_range_max))\n",
    "        while np.sqrt(state_vector[1]**2+state_vector[3]**2) < 0.15:\n",
    "            state_vector = StateVector(np.random.uniform(low=state_range_min, high=state_range_max))\n",
    "        meas_range_low = self.meas_range[0]\n",
    "        meas_range_max = self.meas_range[1]\n",
    "        self.measurement_model.translation_offset = np.random.uniform(low=meas_range_low, high=meas_range_max).reshape(len(meas_range_low),1)\n",
    "        #process_noise = \n",
    "        #self.transition_model. = np.randint(low=0,high=len(noise_range))\n",
    "        \n",
    "        return State(state_vector = state_vector, timestamp=time_stamp)\n",
    "\n",
    "    def generate_training_data(self, time_span):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ground_truth : :class:`~.GroundTruthPath`\n",
    "            StateMutableSequence type object used to store ground truth.\n",
    "        initial_state : :class:`~.State`\n",
    "            Initial state for the ground truth system. This MUST be a State,\n",
    "            not a State subclass, like GaussianState or EnsembleState.\n",
    "        prior : :class:`~.GaussianState` or :class:`~.EnsembleState`\n",
    "            Initial state prediction of tracking algorithm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        track : :class:`~.Track`\n",
    "            The Stone Soup track object which contains the list of updated \n",
    "            state predictions made by the tracking algorithm employed.\n",
    "        \"\"\"\n",
    "\n",
    "        #Simulate Measurements\n",
    "        initial_state = self._initializer(time_span[0])\n",
    "        ground_truth = self._generate_ground_truth(initial_state, time_span)\n",
    "        measurements = self._simulate_measurements(ground_truth)\n",
    "        \n",
    "        return ground_truth, measurements\n",
    "\n",
    "    def simulate_track(self, predictor, updater, initial_state, prior, time_span):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        predictor : :class:`~.Predictor`\n",
    "            The Stone Soup predictor to be used.\n",
    "        updater : :class:`~.Predictor`\n",
    "            The Updater to be used.\n",
    "        ground_truth : :class:`~.GroundTruthPath`\n",
    "            StateMutableSequence type object used to store ground truth.\n",
    "        initial_state : :class:`~.State`\n",
    "            Initial state for the ground truth system. This MUST be a State,\n",
    "            not a State subclass, like GaussianState or EnsembleState.\n",
    "        prior : :class:`~.GaussianState` or :class:`~.EnsembleState`\n",
    "            Initial state prediction of tracking algorithm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        track : :class:`~.Track`\n",
    "            The Stone Soup track object which contains the list of updated \n",
    "            state predictions made by the tracking algorithm employed.\n",
    "        \"\"\"\n",
    "\n",
    "        #Simulate Measurements\n",
    "        ground_truth = self._generate_ground_truth(initial_state, time_span)\n",
    "        measurements = self._simulate_measurements(ground_truth)\n",
    "        \n",
    "        #Initialize Loop Variables\n",
    "        track = Track()\n",
    "        for measurement in measurements:\n",
    "            prediction = predictor.predict(prior, timestamp=measurement.timestamp)\n",
    "            hypothesis = SingleHypothesis(prediction, measurement)  # Group a prediction and measurement\n",
    "            posterior = updater.update(hypothesis)\n",
    "            track.append(posterior)\n",
    "            prior = track[-1]\n",
    "        return ground_truth, track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89b226e-0314-4d3b-ad70-5381641097e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "from stonesoup.types.array import StateVector, CovarianceMatrix\n",
    "from stonesoup.types.state import State, GaussianState\n",
    "\n",
    "from stonesoup.models.transition.linear import (CombinedLinearGaussianTransitionModel,\n",
    "                                                ConstantVelocity)\n",
    "from stonesoup.models.measurement.linear import LinearGaussian\n",
    "from stonesoup.updater.kalman import KalmanUpdater, ExtendedKalmanUpdater\n",
    "from stonesoup.predictor.ensemble import EnsemblePredictor\n",
    "from stonesoup.predictor.kalman import KalmanPredictor, ExtendedKalmanPredictor\n",
    "from stonesoup.models.measurement.nonlinear import CartesianToBearingRange\n",
    "\n",
    "\"\"\"\n",
    "    This script represents the code used to gather the data used in [PAPER HERE].\n",
    "    \n",
    "    This repository is structured such that different stone soup algorithms \n",
    "    can be run rapidly. Hopefully I've made it modular enough to \n",
    "    allow swapping of things like algorithms, and \"experiments\" by replacing\n",
    "    the desired transition and measurement models.\n",
    "    \n",
    "    The simulator class requires a transition and \n",
    "    measurement model, then the simulate_track method accepts a Stone Soup\n",
    "    Predictor, Updater, ground truth initial state, initial state for the\n",
    "    chosen algorithm, and a span of time which the simulation takes place over.\n",
    "    This time span should be an evenly spaced datetime.datetime list.\n",
    "    \n",
    "    The simulator then, is used to gather \"Track\" instances, and with a list \n",
    "    of tracks, RMSE can then be calculated.\n",
    "\"\"\"\n",
    "\n",
    "i = 60\n",
    "num_vectors = i*5\n",
    "\n",
    "\"\"\"\n",
    "    Here, we get our initial variables for simulation. For this, we are just\n",
    "    using a time span of 60 time instances spaced one second apart.\n",
    "\"\"\"\n",
    "\n",
    "timestamp = datetime.datetime(2021, 4, 2, 12, 28, 57)\n",
    "tMax = 30\n",
    "dt = 1\n",
    "tRange = tMax // dt\n",
    "plot_time_span = np.array([dt*i for i in range(tRange)])\n",
    "\n",
    "time_span = np.array([timestamp + datetime.timedelta(seconds=dt*i) for i in range(tRange)])\n",
    "\n",
    "\"\"\"\n",
    "Here we instantiate our transition and measurement models. These are the \n",
    "same models used in the StoneSoup Kalman Filter examples.\n",
    "\"\"\"\n",
    "\n",
    "q_x = 0.05\n",
    "q_y = 0.05\n",
    "sensor_x = 0  # Placing the sensor off-centre\n",
    "sensor_y = 0\n",
    "\n",
    "transition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(q_x),\n",
    "                                                          ConstantVelocity(q_y)])\n",
    "\n",
    "measurement_model = CartesianToBearingRange(\n",
    "ndim_state=4,\n",
    "mapping=(0, 2),\n",
    "noise_covar=np.diag([np.radians(1)**2, 0.1**2]),  # Covariance matrix. 0.2 degree variance in\n",
    "# bearing and 1 metre in range\n",
    "translation_offset=np.array([[sensor_x], [sensor_y]])  # Offset measurements to location of\n",
    "# sensor in cartesian.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3c7b9-e997-4aab-a103-a504880da933",
   "metadata": {},
   "source": [
    "In order to use this simulated data, we need to format it such that it can be loaded into PyTorch easily. Pytorch requires us to specify functions \"__get_item__\" and \"__len__\" in order to be usable by the generic Data Loader. I simply included the init and a helper function to append trajectories to the dataset for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89b76935-2e6b-46e2-9712-e3bb6fb1815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Datasets should be specified on a per simulation basis\n",
    "\n",
    "#class sequence_generator(Dataset):\n",
    "\n",
    "class dataset_2D_bearing_range(Dataset):\n",
    "    #Range Bearing 2D Dataset Sequence Packer\n",
    "\n",
    "    def __init__(self, dataset, ground_truth=None, measurements=None):\n",
    "        if dataset == None:   \n",
    "            gt = np.array([e.state_vector for e in ground_truth]).squeeze().T\n",
    "            gt = torch.tensor(gt.astype(dtype=np.float32),device = device)\n",
    "            ms = np.array([m.state_vector for m in measurements]).squeeze().T\n",
    "            ms = torch.tensor(ms.astype(dtype=np.float32),device = device)\n",
    "            self.dataset = torch.unsqueeze(torch.cat((ms,gt)),dim=0)\n",
    "        else:\n",
    "            self.dataset = dataset\n",
    "            \n",
    "    \n",
    "    def append_to_dataset(self, ground_truth, measurements):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        ground_truth : :class:`~.list`\n",
    "            A list of Stonesoup States.\n",
    "        measurements : :class:`~.measurements`\n",
    "            The list of Stonesoup Measurements to be used.\n",
    "        \"\"\"\n",
    "    \n",
    "        gt = np.array([e.state_vector for e in ground_truth]).squeeze().T\n",
    "        gt = torch.tensor(gt.astype(dtype=np.float32),device = device)\n",
    "        ms = np.array([m.state_vector for m in measurements]).squeeze().T\n",
    "        ms = torch.tensor(ms.astype(dtype=np.float32),device = device)\n",
    "        new_entry = torch.cat((ms,gt),dim=0)\n",
    "\n",
    "        self.dataset = torch.cat((self.dataset,torch.unsqueeze(new_entry,dim=0)),dim=0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx,0:2].T, self.dataset[idx,2:6].T\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f5f341-04aa-4f3f-b554-c01f40df6165",
   "metadata": {},
   "source": [
    "This next module is something I call the Composer. The Composer's job is to return a useful dataset for use in PyTorch, and I will tell it how many trajectories I want, and it will call the relevant simulator and dataset methods to generate a dataset for the purpose of learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35ce240c-0a77-4470-b1b1-5230d611b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Datasets should be specified on a per simulation basis\n",
    "# Should look up realistic values for radar precision and initial states\n",
    "# Come up with realistic constraints for initial velocities\n",
    "\n",
    "#class sequence_generator(Dataset):\n",
    "\n",
    "class dataset_composer():\n",
    "    #Range Bearing 2D Dataset Sequence Packer\n",
    "    def __init__(self, simulators, training_dataset, testing_dataset, batch_size, num_workers):\n",
    "        #pass a list of simulators\n",
    "        self.simulators = simulators\n",
    "        self.training_dataset = training_dataset\n",
    "        self.testing_dataset = testing_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def simulate_trajectories(self, training_dataset_len, testing_ratio):\n",
    "        for simulator in self.simulators:\n",
    "            trainset_length = int((training_dataset_len-1) / len(self.simulators)) # Partition dataset into sets from each simulator\n",
    "            for i in range(trainset_length):\n",
    "                ground_truth, measurements = simulator.generate_training_data(time_span)\n",
    "                self.training_dataset.append_to_dataset(ground_truth, measurements)\n",
    "            trainloader = DataLoader(self.training_dataset)\n",
    "\n",
    "            testset_length = int(((training_dataset_len)*testing_ratio -1)/len(self.simulators))\n",
    "            for i in range(testset_length):\n",
    "                ground_truth, measurements = simulator.generate_training_data(time_span)\n",
    "                self.testing_dataset.append_to_dataset(ground_truth, measurements)\n",
    "            testloader = DataLoader(self.testing_dataset)\n",
    "        return trainloader, testloader\n",
    "\n",
    "    def output_to_file(self, trainset_name, testset_name):\n",
    "        torch.save(self.training_dataset, trainset_name)\n",
    "        torch.save(self.testing_dataset, testset_name)\n",
    "\n",
    "    def read_from_file(self, training_file_name, testing_file_name, device):\n",
    "        trn = torch.load(training_file_name, map_location = device)\n",
    "        tst = torch.load(testing_file_name, map_location = device)\n",
    "        return DataLoader(trn, batch_size = self.batch_size, num_workers=self.num_workers), DataLoader(tst, batch_size = self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6503acaa-e633-4dbf-b51f-2123301dd2c4",
   "metadata": {},
   "source": [
    "Here we instantiate the simulator, provide a range of permissable values for the initial conditions, noise coefficients, and specify the transition model, and measurement model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3514489-8f13-464a-8c87-296cb40cf9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position, Velocity, Position, Velocity\n",
    "state_range_min = np.array([-15, -2, -15, -2])\n",
    "state_range_max = np.array([15, 2, 15, 2])\n",
    "\n",
    "# Add some sort of miniumum speed\n",
    "\n",
    "meas_range_min = np.array([0, 0])\n",
    "meas_range_max = np.array([0, 0])\n",
    "\n",
    "# Randomization for process noise\n",
    "#process_noise_coefficients = [0.005, 0.05, 0.5]\n",
    "process_noise_coefficients = [0.05]\n",
    "\n",
    "nonlinear_simulator = simulator(transition_model=transition_model,\n",
    "                      measurement_model=measurement_model,\n",
    "                      state_range = (state_range_min, state_range_max),\n",
    "                      meas_range = (meas_range_min, meas_range_max)\n",
    ")\n",
    "\n",
    "ground_truth, measurements = nonlinear_simulator.generate_training_data(time_span)\n",
    "training_dataset =  dataset_2D_bearing_range(dataset = None, ground_truth = ground_truth, measurements = measurements)\n",
    "testing_dataset =  dataset_2D_bearing_range(dataset = None, ground_truth = ground_truth, measurements = measurements)\n",
    "\n",
    "#training_dataset_len = 1000000 #  1,000,000\n",
    "training_dataset_len = 100000 #  100,000\n",
    "#training_dataset_len = 10000\n",
    "testing_ratio = 0.15\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 0\n",
    "\n",
    "dataset_composer = dataset_composer([nonlinear_simulator], testing_dataset, training_dataset, batch_size = batch_size, num_workers = num_workers)\n",
    "\n",
    "#trainloader, testloader = dataset_composer.simulate_trajectories(training_dataset_len, testing_ratio)\n",
    "#dataset_composer.output_to_file('training_data','testing_data')\n",
    "trainloader, testloader = dataset_composer.read_from_file('training_data', 'testing_data', device)\n",
    "#print('Training Dataset Length:', training_dataset.__len__())\n",
    "#print('Testing Dataset Length:',testing_dataset.__len__())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f298b99-0c26-4436-a6a3-bc227fd8b9fb",
   "metadata": {},
   "source": [
    "Turn Sequences into dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ad44e-5074-4c21-8bbe-7a2968ccc9f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf9492f-f6d9-4dba-b63d-390ca68b5639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 32\n",
    "num_layers = 1\n",
    "nonlinearity = 'relu'\n",
    "output_size = 4\n",
    "\n",
    "class UniDirectionalRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, nonlinearity, num_layers, device):\n",
    "        super(UniDirectionalRNN, self).__init__()\n",
    "        self.RNN = torch.nn.RNN(input_size, hidden_size, bidirectional=False, num_layers=num_layers, device=device)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size, device=device)\n",
    "\n",
    "    def forward(self, input, RNN_hidden):\n",
    "        RNN_output, RNN_hidden = self.RNN(input, RNN_hidden)\n",
    "        output = self.linear(RNN_output)\n",
    "        return output, RNN_hidden\n",
    "\n",
    "class BiDirectionalRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, nonlinearity, num_layers, device):\n",
    "        super(BiDirectionalRNN, self).__init__()\n",
    "        self.RNN = torch.nn.RNN(input_size, hidden_size, bidirectional=True, num_layers=num_layers, device=device)\n",
    "        self.linear = torch.nn.Linear(2*hidden_size, output_size, device=device)\n",
    "\n",
    "    def forward(self, input, RNN_hidden):\n",
    "        RNN_output, RNN_hidden = self.RNN(input, RNN_hidden)\n",
    "        output = self.linear(RNN_output)\n",
    "        return output, RNN_hidden\n",
    "\n",
    "class BiDirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, device):\n",
    "        super(BiDirectionalLSTM, self).__init__()\n",
    "        self.LSTM = torch.nn.LSTM(input_size, hidden_size, bidirectional=True, num_layers=num_layers, device=device)\n",
    "        self.linear = torch.nn.Linear(2*hidden_size, output_size, device=device)\n",
    "\n",
    "    def forward(self, input, LSTM_hidden, LSTM_cell):\n",
    "        LSTM_output, (LSTM_hidden, LSTM_cell) = self.LSTM(input, (LSTM_hidden, LSTM_cell))\n",
    "        output = self.linear(LSTM_output)\n",
    "        return output, (LSTM_hidden, LSTM_cell)\n",
    "\n",
    "class UniDirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, device):\n",
    "        super(UniDirectionalLSTM, self).__init__()\n",
    "        self.LSTM = torch.nn.LSTM(input_size, hidden_size, bidirectional=False, num_layers=num_layers, device=device)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size, device=device)\n",
    "\n",
    "    def forward(self, input, LSTM_hidden, LSTM_cell):\n",
    "        LSTM_output, (LSTM_hidden, LSTM_cell) = self.LSTM(input, (LSTM_hidden, LSTM_cell))\n",
    "        output = self.linear(LSTM_output)\n",
    "        return output, (LSTM_hidden, LSTM_cell)\n",
    "\n",
    "class MHATransformer(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=16, output_size = 4, nhead=4, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, device=None):\n",
    "        super(MHATransformer, self).__init__()\n",
    "        self.input_layer = torch.nn.Linear(input_size, hidden_size, device=device)\n",
    "        self.transformer = torch.nn.Transformer(d_model=hidden_size, nhead=nhead,\n",
    "                                                num_encoder_layers=num_encoder_layers,\n",
    "                                                num_decoder_layers=num_decoder_layers,\n",
    "                                                dim_feedforward=dim_feedforward,\n",
    "                                                batch_first=True,\n",
    "                                                device=device)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, output_size, device=device)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden_input = self.input_layer(input)\n",
    "        transformer_output = self.transformer(hidden_input, hidden_input)\n",
    "        return self.output_layer(transformer_output)\n",
    "\n",
    "UniRNN = UniDirectionalRNN(input_size=2, hidden_size=32, output_size=4, nonlinearity=nonlinearity, num_layers=1, device=device)\n",
    "BiRNN = BiDirectionalRNN(input_size=2, hidden_size=32, output_size=4, nonlinearity=nonlinearity, num_layers=1, device=device)\n",
    "UniLSTM = UniDirectionalLSTM(input_size=2, hidden_size=32, output_size=4, num_layers=1, device=device)\n",
    "BiLSTM = BiDirectionalLSTM(input_size=2, hidden_size=32, output_size=4, num_layers=1, device=device)\n",
    "Transformer = MHATransformer(input_size=2, hidden_size = 32, output_size = 4, nhead=16, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d4ce5-9a2e-4efd-972b-4a73124ee52d",
   "metadata": {},
   "source": [
    "Initialize Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "985381c6-2a9f-435e-a58c-57de3ce79901",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "UniRNN_learning_rate = 0.00005;\n",
    "UniRNN_weight_decay = 0.000 #L2 Regularization Factor\n",
    "UniRNN_optimizer = torch.optim.Adam(UniRNN.parameters(),lr=UniRNN_learning_rate, weight_decay = UniRNN_weight_decay)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "BiRNN_learning_rate = 0.00005;\n",
    "BiRNN_weight_decay = 0.000 #L2 Regularization Factor\n",
    "BiRNN_optimizer = torch.optim.Adam(BiRNN.parameters(),lr=BiRNN_learning_rate, weight_decay = BiRNN_learning_rate)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "UniLSTM_learning_rate = 0.00005;\n",
    "UniLSTM_weight_decay = 0.000 #L2 Regularization Factor\n",
    "UniLSTM_optimizer = torch.optim.Adam(UniLSTM.parameters(),lr=UniLSTM_learning_rate, weight_decay = UniLSTM_weight_decay)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "BiLSTM_learning_rate = 0.00005;\n",
    "BiLSTM_weight_decay = 0.000 #L2 Regularization Factor\n",
    "BiLSTM_optimizer = torch.optim.Adam(BiLSTM.parameters(),lr=BiLSTM_learning_rate, weight_decay = BiLSTM_weight_decay)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "TransformerLearning_rate = 0.000005;\n",
    "TransformerWeight_decay = 0.000 #L2 Regularization Factor\n",
    "Transformer_optimizer = torch.optim.Adam(Transformer.parameters(),lr=TransformerLearning_rate, weight_decay = TransformerWeight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214fb703-df50-4988-b69c-9bfa2041e242",
   "metadata": {},
   "source": [
    "Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba3b1a3-f789-4b2d-a3db-d16fd2c4db3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "214a3363-ee43-4566-945f-a1df775924f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\echo4\\.conda\\envs\\rnn\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 1         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    319.534  :   282.721  -----\n",
      "-----   BiDirectional RNN:   -----    319.201  :   282.365  -----\n",
      "-----  UniDirectional LSTM:  -----    320.459  :   283.873  -----\n",
      "-----  BiDirectional LSTM:   -----    320.610  :   283.538  -----\n",
      "-----      Transformer:      -----    259.693  :   229.338  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 2         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    316.243  :   278.740  -----\n",
      "-----   BiDirectional RNN:   -----    314.915  :   277.392  -----\n",
      "-----  UniDirectional LSTM:  -----    317.901  :   280.521  -----\n",
      "-----  BiDirectional LSTM:   -----    313.172  :   276.182  -----\n",
      "-----      Transformer:      -----    238.806  :   210.483  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 3         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    311.367  :   273.541  -----\n",
      "-----   BiDirectional RNN:   -----    308.637  :   270.777  -----\n",
      "-----  UniDirectional LSTM:  -----    313.206  :   275.577  -----\n",
      "-----  BiDirectional LSTM:   -----    303.806  :   267.163  -----\n",
      "-----      Transformer:      -----    218.924  :   192.631  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 4         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    306.007  :   267.613  -----\n",
      "-----   BiDirectional RNN:   -----    300.564  :   262.432  -----\n",
      "-----  UniDirectional LSTM:  -----    307.986  :   270.342  -----\n",
      "-----  BiDirectional LSTM:   -----    294.532  :   257.743  -----\n",
      "-----      Transformer:      -----    199.143  :   175.419  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 5         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    300.191  :   261.716  -----\n",
      "-----   BiDirectional RNN:   -----    292.335  :   253.589  -----\n",
      "-----  UniDirectional LSTM:  -----    302.958  :   265.337  -----\n",
      "-----  BiDirectional LSTM:   -----    286.233  :   249.615  -----\n",
      "-----      Transformer:      -----    180.867  :   159.048  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 6         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    294.141  :   255.748  -----\n",
      "-----   BiDirectional RNN:   -----    283.977  :   245.237  -----\n",
      "-----  UniDirectional LSTM:  -----    298.170  :   260.632  -----\n",
      "-----  BiDirectional LSTM:   -----    278.009  :   241.487  -----\n",
      "-----      Transformer:      -----    163.416  :   143.631  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 7         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    287.784  :   249.404  -----\n",
      "-----   BiDirectional RNN:   -----    275.887  :   237.313  -----\n",
      "-----  UniDirectional LSTM:  -----    293.364  :   255.919  -----\n",
      "-----  BiDirectional LSTM:   -----    270.083  :   233.823  -----\n",
      "-----      Transformer:      -----    146.988  :   129.108  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 8         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    280.832  :   243.074  -----\n",
      "-----   BiDirectional RNN:   -----    267.995  :   229.762  -----\n",
      "-----  UniDirectional LSTM:  -----    288.557  :   251.235  -----\n",
      "-----  BiDirectional LSTM:   -----    262.347  :   226.505  -----\n",
      "-----      Transformer:      -----    131.308  :   115.439  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 9         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    274.305  :   236.928  -----\n",
      "-----   BiDirectional RNN:   -----    260.039  :   222.400  -----\n",
      "-----  UniDirectional LSTM:  -----    283.722  :   246.557  -----\n",
      "-----  BiDirectional LSTM:   -----    254.592  :   219.360  -----\n",
      "-----      Transformer:      -----    116.174  :   102.871  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 10         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    267.910  :   230.985  -----\n",
      "-----   BiDirectional RNN:   -----    252.462  :   215.404  -----\n",
      "-----  UniDirectional LSTM:  -----    278.832  :   241.894  -----\n",
      "-----  BiDirectional LSTM:   -----    246.792  :   212.306  -----\n",
      "-----      Transformer:      -----    102.586  :    91.052  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 11         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    261.605  :   225.246  -----\n",
      "-----   BiDirectional RNN:   -----    245.156  :   208.789  -----\n",
      "-----  UniDirectional LSTM:  -----    273.732  :   237.248  -----\n",
      "-----  BiDirectional LSTM:   -----    239.072  :   205.383  -----\n",
      "-----      Transformer:      -----     89.923  :    80.169  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 12         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    255.429  :   219.733  -----\n",
      "-----   BiDirectional RNN:   -----    238.132  :   202.535  -----\n",
      "-----  UniDirectional LSTM:  -----    267.484  :   231.893  -----\n",
      "-----  BiDirectional LSTM:   -----    231.468  :   198.586  -----\n",
      "-----      Transformer:      -----     78.268  :    70.286  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 13         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    249.424  :   214.467  -----\n",
      "-----   BiDirectional RNN:   -----    231.407  :   196.631  -----\n",
      "-----  UniDirectional LSTM:  -----    262.317  :   227.128  -----\n",
      "-----  BiDirectional LSTM:   -----    223.650  :   191.734  -----\n",
      "-----      Transformer:      -----     67.616  :    61.355  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 14         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    243.627  :   209.464  -----\n",
      "-----   BiDirectional RNN:   -----    224.982  :   191.068  -----\n",
      "-----  UniDirectional LSTM:  -----    257.269  :   222.498  -----\n",
      "-----  BiDirectional LSTM:   -----    215.417  :   184.955  -----\n",
      "-----      Transformer:      -----     58.017  :    53.119  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 15         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    238.067  :   204.733  -----\n",
      "-----   BiDirectional RNN:   -----    218.837  :   185.829  -----\n",
      "-----  UniDirectional LSTM:  -----    252.257  :   217.944  -----\n",
      "-----  BiDirectional LSTM:   -----    207.583  :   178.378  -----\n",
      "-----      Transformer:      -----     49.152  :    45.874  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 16         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    232.760  :   200.278  -----\n",
      "-----   BiDirectional RNN:   -----    212.939  :   180.890  -----\n",
      "-----  UniDirectional LSTM:  -----    247.281  :   213.451  -----\n",
      "-----  BiDirectional LSTM:   -----    199.983  :   171.952  -----\n",
      "-----      Transformer:      -----     41.115  :    39.401  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 17         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    227.718  :   196.099  -----\n",
      "-----   BiDirectional RNN:   -----    207.246  :   176.217  -----\n",
      "-----  UniDirectional LSTM:  -----    242.331  :   209.006  -----\n",
      "-----  BiDirectional LSTM:   -----    192.439  :   165.609  -----\n",
      "-----      Transformer:      -----     34.947  :    33.840  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 18         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    222.941  :   192.198  -----\n",
      "-----   BiDirectional RNN:   -----    201.702  :   171.758  -----\n",
      "-----  UniDirectional LSTM:  -----    237.391  :   204.599  -----\n",
      "-----  BiDirectional LSTM:   -----    184.916  :   159.324  -----\n",
      "-----      Transformer:      -----     28.725  :    28.933  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 19         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    218.429  :   188.571  -----\n",
      "-----   BiDirectional RNN:   -----    196.245  :   167.455  -----\n",
      "-----  UniDirectional LSTM:  -----    232.442  :   200.216  -----\n",
      "-----  BiDirectional LSTM:   -----    177.446  :   153.094  -----\n",
      "-----      Transformer:      -----     23.498  :    24.496  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 20         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    214.175  :   185.214  -----\n",
      "-----   BiDirectional RNN:   -----    190.829  :   163.261  -----\n",
      "-----  UniDirectional LSTM:  -----    227.479  :   195.850  -----\n",
      "-----  BiDirectional LSTM:   -----    170.063  :   146.935  -----\n",
      "-----      Transformer:      -----     19.126  :    20.732  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 21         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    210.165  :   182.114  -----\n",
      "-----   BiDirectional RNN:   -----    185.442  :   159.154  -----\n",
      "-----  UniDirectional LSTM:  -----    222.506  :   191.498  -----\n",
      "-----  BiDirectional LSTM:   -----    162.784  :   140.871  -----\n",
      "-----      Transformer:      -----     15.718  :    17.621  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 22         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    206.377  :   179.253  -----\n",
      "-----   BiDirectional RNN:   -----    180.097  :   155.131  -----\n",
      "-----  UniDirectional LSTM:  -----    217.524  :   187.154  -----\n",
      "-----  BiDirectional LSTM:   -----    155.624  :   134.928  -----\n",
      "-----      Transformer:      -----     12.436  :    14.984  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 23         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    202.776  :   176.597  -----\n",
      "-----   BiDirectional RNN:   -----    174.823  :   151.195  -----\n",
      "-----  UniDirectional LSTM:  -----    212.525  :   182.820  -----\n",
      "-----  BiDirectional LSTM:   -----    148.608  :   129.128  -----\n",
      "-----      Transformer:      -----     10.055  :    12.636  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 24         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    199.315  :   174.105  -----\n",
      "-----   BiDirectional RNN:   -----    169.635  :   147.350  -----\n",
      "-----  UniDirectional LSTM:  -----    207.516  :   178.507  -----\n",
      "-----  BiDirectional LSTM:   -----    141.766  :   123.489  -----\n",
      "-----      Transformer:      -----      8.277  :    10.988  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 25         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    195.941  :   171.728  -----\n",
      "-----   BiDirectional RNN:   -----    164.538  :   143.595  -----\n",
      "-----  UniDirectional LSTM:  -----    202.527  :   174.231  -----\n",
      "-----  BiDirectional LSTM:   -----    135.127  :   118.023  -----\n",
      "-----      Transformer:      -----      6.611  :     9.319  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 26         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    192.614  :   169.428  -----\n",
      "-----   BiDirectional RNN:   -----    159.545  :   139.927  -----\n",
      "-----  UniDirectional LSTM:  -----    197.584  :   170.007  -----\n",
      "-----  BiDirectional LSTM:   -----    128.708  :   112.738  -----\n",
      "-----      Transformer:      -----      5.833  :     8.020  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 27         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    189.314  :   167.186  -----\n",
      "-----   BiDirectional RNN:   -----    154.673  :   136.336  -----\n",
      "-----  UniDirectional LSTM:  -----    192.700  :   165.844  -----\n",
      "-----  BiDirectional LSTM:   -----    122.517  :   107.637  -----\n",
      "-----      Transformer:      -----      5.210  :     7.101  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 28         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    186.038  :   164.992  -----\n",
      "-----   BiDirectional RNN:   -----    149.929  :   132.806  -----\n",
      "-----  UniDirectional LSTM:  -----    187.888  :   161.748  -----\n",
      "-----  BiDirectional LSTM:   -----    116.562  :   102.718  -----\n",
      "-----      Transformer:      -----      4.525  :     6.120  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 29         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    182.794  :   162.843  -----\n",
      "-----   BiDirectional RNN:   -----    145.305  :   129.322  -----\n",
      "-----  UniDirectional LSTM:  -----    183.154  :   157.722  -----\n",
      "-----  BiDirectional LSTM:   -----    110.845  :    97.977  -----\n",
      "-----      Transformer:      -----      3.952  :     5.572  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 30         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    179.593  :   160.741  -----\n",
      "-----   BiDirectional RNN:   -----    140.795  :   125.875  -----\n",
      "-----  UniDirectional LSTM:  -----    178.502  :   153.768  -----\n",
      "-----  BiDirectional LSTM:   -----    105.364  :    93.407  -----\n",
      "-----      Transformer:      -----      3.875  :     5.059  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 31         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    176.446  :   158.685  -----\n",
      "-----   BiDirectional RNN:   -----    136.397  :   122.465  -----\n",
      "-----  UniDirectional LSTM:  -----    173.936  :   149.885  -----\n",
      "-----  BiDirectional LSTM:   -----    100.115  :    89.007  -----\n",
      "-----      Transformer:      -----      3.227  :     4.505  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 32         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    173.362  :   156.676  -----\n",
      "-----   BiDirectional RNN:   -----    132.115  :   119.100  -----\n",
      "-----  UniDirectional LSTM:  -----    169.456  :   146.076  -----\n",
      "-----  BiDirectional LSTM:   -----     95.089  :    84.777  -----\n",
      "-----      Transformer:      -----      3.070  :     4.136  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 33         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    170.347  :   154.711  -----\n",
      "-----   BiDirectional RNN:   -----    127.948  :   115.787  -----\n",
      "-----  UniDirectional LSTM:  -----    165.060  :   142.340  -----\n",
      "-----  BiDirectional LSTM:   -----     90.281  :    80.719  -----\n",
      "-----      Transformer:      -----      2.973  :     3.871  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 34         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    167.404  :   152.790  -----\n",
      "-----   BiDirectional RNN:   -----    123.894  :   112.534  -----\n",
      "-----  UniDirectional LSTM:  -----    160.742  :   138.679  -----\n",
      "-----  BiDirectional LSTM:   -----     85.682  :    76.833  -----\n",
      "-----      Transformer:      -----      2.745  :     3.809  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 35         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    164.535  :   150.909  -----\n",
      "-----   BiDirectional RNN:   -----    119.950  :   109.347  -----\n",
      "-----  UniDirectional LSTM:  -----    156.499  :   135.092  -----\n",
      "-----  BiDirectional LSTM:   -----     81.284  :    73.117  -----\n",
      "-----      Transformer:      -----      2.711  :     3.547  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 36         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    161.740  :   149.064  -----\n",
      "-----   BiDirectional RNN:   -----    116.110  :   106.227  -----\n",
      "-----  UniDirectional LSTM:  -----    152.328  :   131.580  -----\n",
      "-----  BiDirectional LSTM:   -----     77.078  :    69.572  -----\n",
      "-----      Transformer:      -----      2.796  :     3.402  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 37         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    159.017  :   147.249  -----\n",
      "-----   BiDirectional RNN:   -----    112.373  :   103.173  -----\n",
      "-----  UniDirectional LSTM:  -----    148.232  :   128.144  -----\n",
      "-----  BiDirectional LSTM:   -----     73.062  :    66.192  -----\n",
      "-----      Transformer:      -----      2.975  :     3.503  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 38         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    156.362  :   145.456  -----\n",
      "-----   BiDirectional RNN:   -----    108.736  :   100.183  -----\n",
      "-----  UniDirectional LSTM:  -----    144.217  :   124.783  -----\n",
      "-----  BiDirectional LSTM:   -----     69.228  :    62.972  -----\n",
      "-----      Transformer:      -----      2.854  :     3.204  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 39         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    153.770  :   143.676  -----\n",
      "-----   BiDirectional RNN:   -----    105.195  :    97.256  -----\n",
      "-----  UniDirectional LSTM:  -----    140.287  :   121.501  -----\n",
      "-----  BiDirectional LSTM:   -----     65.575  :    59.906  -----\n",
      "-----      Transformer:      -----      2.873  :     3.396  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 40         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    151.230  :   141.902  -----\n",
      "-----   BiDirectional RNN:   -----    101.749  :    94.391  -----\n",
      "-----  UniDirectional LSTM:  -----    136.451  :   118.297  -----\n",
      "-----  BiDirectional LSTM:   -----     62.096  :    56.988  -----\n",
      "-----      Transformer:      -----      2.597  :     2.988  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 41         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    148.735  :   140.126  -----\n",
      "-----   BiDirectional RNN:   -----     98.396  :    91.589  -----\n",
      "-----  UniDirectional LSTM:  -----    132.714  :   115.172  -----\n",
      "-----  BiDirectional LSTM:   -----     58.783  :    54.211  -----\n",
      "-----      Transformer:      -----      2.735  :     2.755  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 42         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    146.278  :   138.343  -----\n",
      "-----   BiDirectional RNN:   -----     95.133  :    88.851  -----\n",
      "-----  UniDirectional LSTM:  -----    129.079  :   112.123  -----\n",
      "-----  BiDirectional LSTM:   -----     55.632  :    51.568  -----\n",
      "-----      Transformer:      -----      2.503  :     2.804  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 43         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    143.859  :   136.554  -----\n",
      "-----   BiDirectional RNN:   -----     91.959  :    86.175  -----\n",
      "-----  UniDirectional LSTM:  -----    125.545  :   109.150  -----\n",
      "-----  BiDirectional LSTM:   -----     52.637  :    49.052  -----\n",
      "-----      Transformer:      -----      3.109  :     2.828  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 44         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    141.477  :   134.765  -----\n",
      "-----   BiDirectional RNN:   -----     88.873  :    83.563  -----\n",
      "-----  UniDirectional LSTM:  -----    122.111  :   106.252  -----\n",
      "-----  BiDirectional LSTM:   -----     49.793  :    46.656  -----\n",
      "-----      Transformer:      -----      2.852  :     2.759  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 45         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    139.130  :   132.983  -----\n",
      "-----   BiDirectional RNN:   -----     85.874  :    81.013  -----\n",
      "-----  UniDirectional LSTM:  -----    118.772  :   103.429  -----\n",
      "-----  BiDirectional LSTM:   -----     47.096  :    44.375  -----\n",
      "-----      Transformer:      -----      2.373  :     2.690  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 46         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    136.817  :   131.214  -----\n",
      "-----   BiDirectional RNN:   -----     82.961  :    78.523  -----\n",
      "-----  UniDirectional LSTM:  -----    115.524  :   100.680  -----\n",
      "-----  BiDirectional LSTM:   -----     44.544  :    42.205  -----\n",
      "-----      Transformer:      -----      2.303  :     2.687  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 47         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    134.539  :   129.462  -----\n",
      "-----   BiDirectional RNN:   -----     80.133  :    76.093  -----\n",
      "-----  UniDirectional LSTM:  -----    112.361  :    98.002  -----\n",
      "-----  BiDirectional LSTM:   -----     42.133  :    40.143  -----\n",
      "-----      Transformer:      -----      2.780  :     2.693  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 48         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    132.293  :   127.730  -----\n",
      "-----   BiDirectional RNN:   -----     77.388  :    73.722  -----\n",
      "-----  UniDirectional LSTM:  -----    109.281  :    95.393  -----\n",
      "-----  BiDirectional LSTM:   -----     39.856  :    38.185  -----\n",
      "-----      Transformer:      -----      2.369  :     2.633  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 49         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    130.080  :   126.019  -----\n",
      "-----   BiDirectional RNN:   -----     74.725  :    71.413  -----\n",
      "-----  UniDirectional LSTM:  -----    106.277  :    92.851  -----\n",
      "-----  BiDirectional LSTM:   -----     37.709  :    36.325  -----\n",
      "-----      Transformer:      -----      2.215  :     2.431  -----\n",
      "----------------------------------------------------------------- \n",
      "-----      Epoch : 50         -----  Train Loss :  Test Loss -----\n",
      "-----  UniDirectional RNN:   -----    127.899  :   124.332  -----\n",
      "-----   BiDirectional RNN:   -----     72.139  :    69.165  -----\n",
      "-----  UniDirectional LSTM:  -----    103.349  :    90.376  -----\n",
      "-----  BiDirectional LSTM:   -----     35.679  :    34.556  -----\n",
      "-----      Transformer:      -----      2.364  :     2.535  -----\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "UniRNN_hidden = torch.zeros((1, tMax, hidden_size),device=device)\n",
    "\n",
    "BiRNN_hidden = torch.zeros((2, tMax, hidden_size),device=device)\n",
    "\n",
    "UniLSTM_hidden = torch.zeros((1, tMax, hidden_size),device=device)\n",
    "UniLSTM_cell = torch.zeros((1, tMax, hidden_size),device=device)\n",
    "\n",
    "BiLSTM_hidden = torch.zeros((2, tMax, hidden_size),device=device)\n",
    "BiLSTM_cell = torch.zeros((2, tMax, hidden_size),device=device)\n",
    "\n",
    "\n",
    "UniRNN_training_loss = torch.zeros(len(trainloader),device=device)\n",
    "BiRNN_training_loss = torch.zeros(len(trainloader),device=device)\n",
    "UniLSTM_training_loss = torch.zeros(len(trainloader),device=device)\n",
    "BiLSTM_training_loss = torch.zeros(len(trainloader),device=device)\n",
    "Transformer_training_loss = torch.zeros(len(trainloader),device=device)\n",
    "\n",
    "UniRNN_testing_loss = torch.zeros(len(testloader),device=device)\n",
    "BiRNN_testing_loss = torch.zeros(len(testloader),device=device)\n",
    "UniLSTM_testing_loss = torch.zeros(len(testloader),device=device)\n",
    "BiLSTM_testing_loss = torch.zeros(len(testloader),device=device)\n",
    "Transformer_testing_loss = torch.zeros(len(testloader),device=device)\n",
    "\n",
    "UniRNN_training_loss_epoch = torch.zeros(n_epochs,device=device)\n",
    "BiRNN_training_loss_epoch = torch.zeros(n_epochs,device=device)\n",
    "UniLSTM_training_loss_epoch = torch.zeros(n_epochs,device=device)\n",
    "BiLSTM_training_loss_epoch = torch.zeros(n_epochs,device=device)\n",
    "Transformer_training_loss_epoch = torch.zeros(n_epochs,device=device)\n",
    "\n",
    "UniRNN_testing_loss_epoch = torch.zeros(n_epochs,device=device)\n",
    "BiRNN_testing_loss_epoch = torch.zeros(n_epochs,device=device)\n",
    "UniLSTM_testing_loss_epoch = torch.zeros(n_epochs,device=device)\n",
    "BiLSTM_testing_loss_epoch = torch.zeros(n_epochs,device=device)\n",
    "Transformer_testing_loss_epoch = torch.zeros(n_epochs,device=device)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    for j, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        UniRNN_optimizer.zero_grad()\n",
    "        BiRNN_optimizer.zero_grad()\n",
    "        UniLSTM_optimizer.zero_grad()\n",
    "        BiLSTM_optimizer.zero_grad()\n",
    "        Transformer_optimizer.zero_grad()\n",
    "        \n",
    "        UniRNN_outputs, UniRNN_hidden = UniRNN(inputs, UniRNN_hidden.detach())\n",
    "        BiRNN_outputs, BiRNN_hidden = BiRNN(inputs, BiRNN_hidden.detach())\n",
    "        UniLSTM_outputs, (UniLSTM_hidden, UniLSTM_cell) = UniLSTM(inputs, UniLSTM_hidden.detach(), UniLSTM_cell.detach())\n",
    "        BiLSTM_outputs, (BiLSTM_hidden, BiLSTM_cell) = BiLSTM(inputs, BiLSTM_hidden.detach(), BiLSTM_cell.detach())\n",
    "        Transformer_outputs = Transformer(inputs)\n",
    "        \n",
    "        UniRNN_train_loss = criterion(UniRNN_outputs, labels)\n",
    "        BiRNN_train_loss = criterion(BiRNN_outputs, labels)\n",
    "        UniLSTM_train_loss = criterion(UniLSTM_outputs, labels)\n",
    "        BiLSTM_train_loss = criterion(BiLSTM_outputs, labels)\n",
    "        Transformer_train_loss = criterion(Transformer_outputs, labels)\n",
    "\n",
    "        UniRNN_training_loss[j] = UniRNN_train_loss\n",
    "        BiRNN_training_loss[j] = BiRNN_train_loss\n",
    "        UniLSTM_training_loss[j] = UniLSTM_train_loss\n",
    "        BiLSTM_training_loss[j] = BiLSTM_train_loss\n",
    "        Transformer_training_loss[j] = Transformer_train_loss\n",
    "        \n",
    "        UniRNN_train_loss.backward()\n",
    "        BiRNN_train_loss.backward()\n",
    "        UniLSTM_train_loss.backward()\n",
    "        BiLSTM_train_loss.backward()\n",
    "        Transformer_train_loss.backward()\n",
    "        \n",
    "        UniRNN_optimizer.step()\n",
    "        BiRNN_optimizer.step()\n",
    "        UniLSTM_optimizer.step()\n",
    "        BiLSTM_optimizer.step()\n",
    "        Transformer_optimizer.step()\n",
    "        \n",
    "    UniRNN_training_loss_epoch[i] = torch.mean(UniRNN_train_loss).item()\n",
    "    BiRNN_training_loss_epoch[i] = torch.mean(BiRNN_train_loss).item()\n",
    "    UniLSTM_training_loss_epoch[i] = torch.mean(UniLSTM_train_loss).item()\n",
    "    BiLSTM_training_loss_epoch[i] = torch.mean(BiLSTM_train_loss).item()\n",
    "    Transformer_training_loss_epoch[i] = torch.mean(Transformer_train_loss).item()\n",
    "    \n",
    "    with (torch.no_grad()):\n",
    "        for j, data in enumerate(testloader):\n",
    "            inputs, labels = data\n",
    "            \n",
    "            UniRNN_outputs, UniRNN_hidden = UniRNN(inputs, UniRNN_hidden.detach())\n",
    "            BiRNN_outputs, BiRNN_hidden = BiRNN(inputs, BiRNN_hidden.detach())\n",
    "            UniLSTM_outputs, (UniLSTM_hidden, UniLSTM_cell) = UniLSTM(inputs, UniLSTM_hidden.detach(), UniLSTM_cell.detach())\n",
    "            BiLSTM_outputs, (BiLSTM_hidden, BiLSTM_cell) = BiLSTM(inputs, BiLSTM_hidden.detach(), BiLSTM_cell.detach())\n",
    "            Transformer_outputs = Transformer(inputs)\n",
    "            \n",
    "            UniRNN_test_loss = criterion(UniRNN_outputs, labels)\n",
    "            BiRNN_test_loss = criterion(BiRNN_outputs, labels)\n",
    "            UniLSTM_test_loss = criterion(UniLSTM_outputs, labels)\n",
    "            BiLSTM_test_loss = criterion(BiLSTM_outputs, labels)\n",
    "            Transformer_test_loss = criterion(Transformer_outputs, labels)\n",
    "\n",
    "            UniRNN_testing_loss[j] = UniRNN_test_loss\n",
    "            BiRNN_testing_loss[j] = BiRNN_test_loss\n",
    "            UniLSTM_testing_loss[j] = UniLSTM_test_loss\n",
    "            BiLSTM_testing_loss[j] = BiLSTM_test_loss\n",
    "            Transformer_testing_loss[j] = Transformer_test_loss\n",
    "            \n",
    "        UniRNN_testing_loss_epoch[i] = torch.mean(UniRNN_testing_loss).item()\n",
    "        BiRNN_testing_loss_epoch[i] = torch.mean(BiRNN_testing_loss).item()\n",
    "        UniLSTM_testing_loss_epoch[i] = torch.mean(UniLSTM_testing_loss).item()\n",
    "        BiLSTM_testing_loss_epoch[i] = torch.mean(BiLSTM_testing_loss).item()\n",
    "        Transformer_testing_loss_epoch[i] = torch.mean(Transformer_testing_loss).item()\n",
    "\n",
    "    print(\"-----------------------------------------------------------------\",\n",
    "          \"\\n-----      Epoch :\",i+1, \"        -----\",\n",
    "          \" Train Loss :  Test Loss -----\")\n",
    "    print(\"-----  UniDirectional RNN:   -----\",\n",
    "          \"{:10.3f}\".format(UniRNN_training_loss_epoch[i].item()) +'  :'+\"{:10.3f}\".format(UniRNN_testing_loss_epoch[i].item())+'  -----')\n",
    "    print(\"-----   BiDirectional RNN:   -----\",\n",
    "          \"{:10.3f}\".format(BiRNN_training_loss_epoch[i].item()) +'  :'+\"{:10.3f}\".format(BiRNN_testing_loss_epoch[i].item())+'  -----')\n",
    "    print(\"-----  UniDirectional LSTM:  -----\",\n",
    "          \"{:10.3f}\".format(UniLSTM_training_loss_epoch[i].item()) +'  :'+\"{:10.3f}\".format(UniLSTM_testing_loss_epoch[i].item())+'  -----')\n",
    "    print(\"-----  BiDirectional LSTM:   -----\",\n",
    "          \"{:10.3f}\".format(BiLSTM_training_loss_epoch[i].item()) +'  :'+\"{:10.3f}\".format(BiLSTM_testing_loss_epoch[i].item())+'  -----')\n",
    "    print(\"-----      Transformer:      -----\",\n",
    "          \"{:10.3f}\".format(Transformer_training_loss_epoch[i].item()) +'  :'+\"{:10.3f}\".format(Transformer_testing_loss_epoch[i].item())+'  -----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29efe313-64a7-47ce-9840-dfcacd5e5a75",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a2a3e-77c0-4611-91a7-c029fe85da1a",
   "metadata": {},
   "source": [
    "Trajectory MSE Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4c174-f4cb-4486-936e-7ec66440f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_mse_func = nn.MSELoss(reduction='none')\n",
    "tst = torch.load('testing_data', map_location = device)\n",
    "testloader = DataLoader(tst)\n",
    "\n",
    "UniRNN_trajectory_MSE = torch.zeros(tMax, output_size, device=device)\n",
    "BiRNN_trajectory_MSE = torch.zeros(tMax, output_size, device=device)\n",
    "UniLSTM_trajectory_MSE = torch.zeros(tMax, output_size, device=device)\n",
    "BiLSTM_trajectory_MSE = torch.zeros(tMax, output_size, device=device)\n",
    "Transformer_trajectory_MSE = torch.zeros(tMax, output_size, device=device)\n",
    "\n",
    "with (torch.no_grad()):\n",
    "    j = 0\n",
    "    for j, data in enumerate(testloader):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        UniRNN_outputs, UniRNN_hidden = UniRNN(inputs, UniRNN_hidden.detach())\n",
    "        BiRNN_outputs, BiRNN_hidden = BiRNN(inputs, BiRNN_hidden.detach())\n",
    "        UniLSTM_outputs, (UniLSTM_hidden, UniLSTM_cell) = UniLSTM(inputs, UniLSTM_hidden.detach(), UniLSTM_cell.detach())\n",
    "        BiLSTM_outputs, (BiLSTM_hidden, BiLSTM_cell) = BiLSTM(inputs, BiLSTM_hidden.detach(), BiLSTM_cell.detach())\n",
    "        Transformer_outputs = Transformer(inputs)\n",
    "        \n",
    "        UniRNN_test_loss = criterion(UniRNN_outputs, labels)\n",
    "        BiRNN_test_loss = criterion(BiRNN_outputs, labels)\n",
    "        UniLSTM_test_loss = criterion(UniLSTM_outputs, labels)\n",
    "        BiLSTM_test_loss = criterion(BiLSTM_outputs, labels)\n",
    "        Transformer_test_loss = criterion(Transformer_outputs, labels)\n",
    "\n",
    "        UniRNN_trajectory_MSE += trajectory_mse_func(UniRNN_outputs.squeeze(), labels.squeeze())\n",
    "        BiRNN_trajectory_MSE += trajectory_mse_func(BiRNN_outputs.squeeze(), labels.squeeze())\n",
    "        UniLSTM_trajectory_MSE += trajectory_mse_func(UniLSTM_outputs.squeeze(), labels.squeeze())\n",
    "        BiLSTM_trajectory_MSE += trajectory_mse_func(BiLSTM_outputs.squeeze(), labels.squeeze())\n",
    "        Transformer_trajectory_MSE += trajectory_mse_func(Transformer_outputs.squeeze(), labels.squeeze())\n",
    "\n",
    "        j += j\n",
    "    UniRNN_testing_MSE = UniRNN_trajectory_MSE / j\n",
    "    BiRNN_testing_MSE = BiRNN_trajectory_MSE / j\n",
    "    UniLSTM_testing_MSE = UniLSTM_trajectory_MSE / j\n",
    "    BiLSTM_testing_MSE = BiLSTM_trajectory_MSE / j\n",
    "    Transformer_testing_MSE = Transformer_trajectory_MSE / j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d1e72-91bb-409c-98e7-dab000104b18",
   "metadata": {},
   "source": [
    "This Plotter class contains all of the info to generate the plots used in the paper. Each method accepts it's own standardized input and handles any data preprocessing internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f50a9-f6cc-4f8a-b241-af7b992722e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class plotter():\n",
    "    def __init__(self):\n",
    "        a = 'Hello World'\n",
    "\n",
    "    def avg_loss_vs_epoch(self, testing_loss):\n",
    "        return 0\n",
    "        \n",
    "    def two_by_one_MSE_vs_time(self, MSE_list, legend_list):\n",
    "        fig, MSE_combined_plot = plt.subplots(1,2)\n",
    "\n",
    "        height = 3\n",
    "        width = 9\n",
    "        \n",
    "        MSE_combined_plot[0].set_xlabel(\"Time (s)\")\n",
    "        MSE_combined_plot[0].set_ylabel(\"Root Mean Square Error (km)\")\n",
    "        MSE_combined_plot[0].set_title(\"Avg RMSE in Position vs Time\")\n",
    "        MSE_combined_plot[0].figure.set_figwidth(width) # 6.4\n",
    "        MSE_combined_plot[0].figure.set_figheight(height) # 6.4\n",
    "        MSE_combined_plot[1].set_xlabel(\"Time (s)\")\n",
    "        MSE_combined_plot[1].set_ylabel(\"Root Mean Square Error (km/s)\")\n",
    "        MSE_combined_plot[1].set_title(\"Avg RMSE in Velocity vs Time\")\n",
    "        MSE_combined_plot[1].figure.set_figwidth(width) # 6.4\n",
    "        MSE_combined_plot[1].figure.set_figheight(height) # 6.4\n",
    "        fig.tight_layout()\n",
    "\n",
    "        t = range(len(time_span))\n",
    "        i = 0\n",
    "        for MSE in MSE_list:\n",
    "            MSE_x = MSE.T[0].cpu().numpy()\n",
    "            MSE_xv = MSE.T[1].cpu().numpy()\n",
    "            MSE_y = MSE.T[2].cpu().numpy()\n",
    "            MSE_yv = MSE.T[3].cpu().numpy()\n",
    "\n",
    "            MSE_pos = np.sqrt( (MSE_x + MSE_y) / 2 )\n",
    "            MSE_vel = np.sqrt( (MSE_xv + MSE_yv) / 2 )\n",
    "\n",
    "            MSE_combined_plot[0].plot(t, MSE_pos)\n",
    "            MSE_combined_plot[1].plot(t, MSE_vel)\n",
    "            MSE_combined_plot[1].legend([legend_list[i]])\n",
    "            i += 1\n",
    "\n",
    "    def two_by_two_MSE_vs_time(self, MSE_list, legend_list):\n",
    "        fig, MSE_combined_plot = plt.subplots(2,2)\n",
    "        scale_factor = 1.5\n",
    "        height = 6 * scale_factor\n",
    "        width = 9 * scale_factor\n",
    "        \n",
    "        MSE_combined_plot[0][0].set_xlabel(\"Time (s)\")\n",
    "        MSE_combined_plot[0][0].set_ylabel(\"Root Mean Square Error (km)\")\n",
    "        MSE_combined_plot[0][0].set_title(\"Avg RMSE in Horizontal Position vs Time\")\n",
    "        MSE_combined_plot[0][0].figure.set_figwidth(width) # 6.4\n",
    "        MSE_combined_plot[0][0].figure.set_figheight(height) # 6.4\n",
    "        \n",
    "        MSE_combined_plot[1][0].set_xlabel(\"Time (s)\")\n",
    "        MSE_combined_plot[1][0].set_ylabel(\"Root Mean Square Error (km)\")\n",
    "        MSE_combined_plot[1][0].set_title(\"Avg RMSE in Vertical Position vs Time\")\n",
    "        MSE_combined_plot[1][0].figure.set_figwidth(width) # 6.4\n",
    "        MSE_combined_plot[1][0].figure.set_figheight(height) # 6.4\n",
    "\n",
    "        MSE_combined_plot[0][1].set_xlabel(\"Time (s)\")\n",
    "        MSE_combined_plot[0][1].set_ylabel(\"Root Mean Square Error (km/s)\")\n",
    "        MSE_combined_plot[0][1].set_title(\"Avg RMSE in Horizontal Velocity vs Time\")\n",
    "        MSE_combined_plot[0][1].figure.set_figwidth(width) # 6.4\n",
    "        MSE_combined_plot[0][1].figure.set_figheight(height) # 6.4\n",
    "        \n",
    "        MSE_combined_plot[1][1].set_xlabel(\"Time (s)\")\n",
    "        MSE_combined_plot[1][1].set_ylabel(\"Root Mean Square Error (km/s)\")\n",
    "        MSE_combined_plot[1][1].set_title(\"Avg RMSE in Vertical Velocity vs Time\")\n",
    "        MSE_combined_plot[1][1].figure.set_figwidth(width) # 6.4\n",
    "        MSE_combined_plot[1][1].figure.set_figheight(height) # 6.4\n",
    "        fig.tight_layout()\n",
    "\n",
    "        t = range(len(time_span))\n",
    "        i = 0\n",
    "        for MSE in MSE_list:\n",
    "            MSE_x = MSE.T[0].cpu().numpy()\n",
    "            MSE_xv = MSE.T[1].cpu().numpy()\n",
    "            MSE_y = MSE.T[2].cpu().numpy()\n",
    "            MSE_yv = MSE.T[3].cpu().numpy()\n",
    "\n",
    "            MSE_combined_plot[0][0].plot(t, MSE_x, label = legend_list[i])\n",
    "            #MSE_combined_plot[0][0].legend([legend_list[i]])\n",
    "            \n",
    "            MSE_combined_plot[1][0].plot(t, MSE_y, label = legend_list[i])\n",
    "            #MSE_combined_plot[1][0].legend([legend_list[i]])\n",
    "            \n",
    "            MSE_combined_plot[0][1].plot(t, MSE_xv, label = legend_list[i])\n",
    "            #MSE_combined_plot[0][1].legend([legend_list[i]])\n",
    "            \n",
    "            MSE_combined_plot[1][1].plot(t, MSE_yv, label = legend_list[i])\n",
    "            #MSE_combined_plot[1][1].legend([legend_list[i]])\n",
    "            i += 1\n",
    "\n",
    "        MSE_combined_plot[0][0].legend(legend_list)\n",
    "        MSE_combined_plot[1][0].legend(legend_list)\n",
    "        MSE_combined_plot[0][1].legend(legend_list)\n",
    "        MSE_combined_plot[1][1].legend(legend_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f94768-f665-41b0-b070-ef71c7b88985",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = plotter()\n",
    "mse_list = [UniRNN_testing_MSE, BiRNN_testing_MSE, UniRNN_testing_MSE, BiRNN_testing_MSE, Transformer_testing_MSE]\n",
    "legend_list = ['Unidirectional RNN', 'Bidirectional RNN', 'Unidirectional LSTM', 'Bidirectional LSTM', 'Transformer']\n",
    "plotter.two_by_one_MSE_vs_time(mse_list, legend_list)\n",
    "plotter.two_by_two_MSE_vs_time(mse_list, legend_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a5fd891-f007-4f65-868a-b085e5d40ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class track_simulator():\n",
    "    def __init__(self, predictor, updater, dataloader, time_span):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        transition_model : :class:`~.Predictor`\n",
    "            The Stone Soup predictor to be used.\n",
    "        measurement_model : :class:`~.Predictor`\n",
    "            The Updater to be used.\n",
    "        \"\"\"\n",
    "        self.predictor = predictor\n",
    "        self.updater = updater\n",
    "        self.dataloader = dataloader\n",
    "        self.time_span = time_span\n",
    "\n",
    "    def initializer(self, time_stamp, labels):\n",
    "        state_range_min = labels[0][0].cpu().numpy()\n",
    "        state_range_max = labels[0][1].cpu().numpy()\n",
    "        state_vector = StateVector(np.random.uniform(low=state_range_min, high=state_range_max))\n",
    "        return GaussianState(state_vector = state_vector, covar = np.eye(4), timestamp=time_stamp)\n",
    "\n",
    "    def _simulate_track(self, input, labels):\n",
    "        track = Track()\n",
    "        prior = self.initializer(self.time_span[0], labels)\n",
    "        for i in range(inputs.shape[1]):\n",
    "            measurement = Detection(state_vector=input[0,i].cpu().numpy(), timestamp = self.time_span[i])\n",
    "            prediction = self.predictor.predict(prior, timestamp=measurement.timestamp)\n",
    "            posterior = self.updater.update(SingleHypothesis(prediction, measurement))\n",
    "            track.append(posterior)\n",
    "            prior = track[-1]\n",
    "        tk = np.array([e.state_vector for e in track]).squeeze().T\n",
    "        tk = torch.tensor(tk.astype(dtype=np.float32),device = device)\n",
    "        return tk.T\n",
    "\n",
    "    def generate_MSE(self):\n",
    "        avg_loss = nn.MSELoss()\n",
    "        trajectory_MSE = nn.MSELoss(reduction='none')\n",
    "        MSE = torch.zeros(1,tMax,4, device=device) # Make this auto detect it\n",
    "        test_loss=0\n",
    "        i = 0\n",
    "        for j, data in enumerate(self.dataloader):\n",
    "            inputs, labels = data\n",
    "            outputs = self._simulate_track(inputs, labels)\n",
    "            test_loss += avg_loss(outputs, labels.squeeze())\n",
    "            MSE += trajectory_MSE(outputs, labels.squeeze())\n",
    "            i += 1\n",
    "        return test_loss / i, MSE.squeeze()/len(self.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf290af-e5c2-45e5-9fce-df06c7278e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "predictor = ExtendedKalmanPredictor(transition_model)\n",
    "updater = ExtendedKalmanUpdater(measurement_model)\n",
    "\n",
    "track_simulator = track_simulator(predictor, updater, testloader, time_span)\n",
    "\n",
    "EKF_epoch_loss, EKF_track = track_simulator.generate_MSE()\n",
    "\n",
    "track_simulator.generate_MSE()\n",
    "\n",
    "epoch_mse, EKF_MSE = track_simulator.generate_MSE()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2359d6-d880-48ab-baf9-2138c7074505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, EKF_mse_plot = plt.subplots(2,2)\n",
    "\n",
    "EKF_x = EKF_MSE.T[0].cpu().numpy()\n",
    "EKF_xv = EKF_MSE.T[1].cpu().numpy()\n",
    "EKF_y = EKF_MSE.T[2].cpu().numpy()\n",
    "EKF_yv = EKF_MSE.T[3].cpu().numpy()\n",
    "t = range(len(time_span))\n",
    "\n",
    "fig.tight_layout()\n",
    "EKF_mse_plot[0][0].plot(t, EKF_x, color=\"C0\")\n",
    "EKF_mse_plot[0][1].plot(t, EKF_xv, color=\"C1\")\n",
    "EKF_mse_plot[1][0].plot(t, EKF_y, color=\"C2\")\n",
    "EKF_mse_plot[1][1].plot(t, EKF_yv, color=\"C3\")\n",
    "#EKFmse[1].set_title(\"Testing Loss (MSE)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1edba1-ff3d-47e5-8e56-4244b785c21c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d54a1f9-b8a1-4321-bee5-8163941625fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea537b1d-fe3c-4c42-b226-8c7ce8c6c27a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
